#!/bin/bash -x

# Usage:
# 1) Set the environment variables below to match your environment/directories
# 2) run_multi.sh ###, where ### is the number of scenes to process
# 3) Check your batch status and/or the L2 output S3 bucket for results

# Batch stack output values here:
AWSStack=espa-batch-costanalysisv3
export espaQueue=espa-batch-costanalysisv3-ESPABatchStack-1CCTGQJ445KBU-ESPA_ProcessJobQueueSpot
export espaJobDefinition=espa-batch-costanalysisv3-ESPABatchStack-1CCTGQJ445KBU-ESPA_ProcessJob

# Region and S3 log values here (note these are for the job submit JSON files, not the espa-processing logs)
export espaJobBucket=jdc-test-dev
export AWSRegion=us-west-2

# Devel host options that are pulled into the job submit JSON file
SQSHOME=/devel/djt/espa-all
export ESPA_PROCESS_TEMPLATE=/devel/djt/espa-all/espa-processing/processing/order_template.json
export ESPA_CONFIG_PATH=/devel/djt/espa-all/espa-container-tools/

# Actual command to run, leave this alone for the most part unless you are updating parameters
# Replace --prefix with --job-file <joblist> and leave off the argument to select all scenes in the file
$SQSHOME/espa-container-tools/SQS/submit_multi.py \
        --batch \
        --bucket mvp-117k-l15-scenes-us-west-2 \
        --queue $AWSQueue \
        --full \
        --prefix L08,L07,L05,L04 \
        $1

